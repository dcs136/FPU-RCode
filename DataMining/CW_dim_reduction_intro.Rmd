---
title: "Dimensionality Reduction"
output: html_notebook
---

Let us load the `tidyverse` package for data transformation and visualization

```{r}

```


## House Prices in Boston

For each neighborhood, a number of variables are given, such as the crime rate, the student/teacher ratio, and the median value of a housing unit in the neighborhood.

The file `BostonHousing.csv` contains information collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes information on 506 census housing tracts in the Boston area. The goal is to _predict the median house price_ in new tracts based on information such as crime rate, pollution, and number of rooms. The response is the median house price (`MEDV)`.

  Variables |  Description
------------|----------------------------------------------------------------
`CRIM`      | Crime rate
`ZN`        | Percentage of residential land zoned for lots over 25,000 ft2
`INDUS`     | Percentage of land occupied by non-retail business
`CHAS`      | Does tract bound Charles River (`= 1` if tract bounds river, `= 0` otherwise)
`NOX`       | Nitric oxide concentration (parts per 10 million)
`RM`        | Average number of rooms per dwelling
`AGE`       | Percentage of owner-occupied units built prior to 1940
`DIS`       | Weighted distances to five Boston employment centers
`RAD`       | Index of accessibility to radial highways
`TAX`       | Full-value property tax rate per $10,000
`PTRATIO`   | Pupil-to-teacher ratio by town
`LSTAT`     | Percentage of lower status of the population
`MEDV`      | Median value of owner-occupied homes in $1000s
`CAT.MEDV`  | Is median value of owner-occupied homes in tract above $30,000 (`CAT.MEDV = 1`) or not (`CAT.MEDV = 0`)


- Read dataset:

```{r, message=FALSE}
housing <- read_csv("https://raw.githubusercontent.com/reisanar/datasets/master/BostonHousing.csv")
```

Print the first 6 observations:

```{r}

```

The first row represents the first neighborhood, which had an average per capita crime rate of 0.006, 18% of the residential land zoned for lots over 25,000 ft2, 2.31% of the land devoted to nonretail business, no border on the Charles River, and so on.

### Data Summaries

Numerical summaries and graphs of the data are very helpful for data reduction. The information that they convey can assist in combining categories of a categorical variable, in choosing variables to remove, in assessing the level of information overlap between variables, and more. 

```{r}
# summary statistics of every feature

```


#### Other summaries

We can also compute the standard deviation, which gives a sense of how dispersed the data are (relative to the mean). Further options, such as `sum(is.na(VARIABLE))`, which gives the number of null values, can tell us about missing values.


```{r}
# data frame with summary statistics




```

Things to notice:

- The mean of the first variable, `CRIM` (as well as several others), is much larger than the median, indicating right skew. 

- None of the variables have missing values. There also do not appear to be indications of extreme values that might result from typing errors.

#### Correlation

Next, we summarize relationships between two or more variables.

For _numerical variables_, we can compute a complete **matrix of correlations** between each pair of variables, using the R function `cor()`.

```{r}
# matrix with correlation coefficients

```

We see that most correlations are low and that many are negative. Pairs that have a very strong (positive or negative) correlation contain a lot of _overlap_ in information and are good candidates for data reduction by removing one of the variables.



Another very useful approach for exploring the data is aggregation by one or more variables. For aggregation by a single variable, we can use `table()`. 

For example, below we show the number of neighborhoods that bound the Charles River vs. those that do not (the variable `CHAS` is chosen as the grouping variable). It appears that the majority of neighborhoods (471 of 506) do not bound the river.

```{r}
# contingency table

```



### Reducing the Number of Categories in Categorical Variables

When a categorical variable has many categories, and this variable is destined to be a predictor, many data mining methods will require converting it into many _dummy variables_. In particular, a variable with $m$ categories will be transformed into either $m$ or $m-1$ dummy variables (depending on the method). This means that even if we have very few original categorical variables, they can greatly inflate the dimension of the dataset.

One way to handle this is to reduce the number of categories by combining close or similar categories. Combining categories requires incorporating expert knowledge and common sense.

Let us compute the proportion of observations for which median value of owner-occupied homes in tract is above $30000 (`CAT.MEDV`), per percentage of residential land zoned for lots over 25,000 ft2 (`ZN`)

```{r}
# calculate proportions, and call them `freq` 
# create new variable `above30k` 

```


We can use the sequence of commands above with the proportions computation, to visualize the distribution of `CAT.MEDV` by `ZN`, by connecting the sequence of instruction above with function calls to `ggplot()` :


```{r}
# putting all pieces together
# summarizing and visualizing




```

In the above figure we see that the distribution of outcome variable `CAT.MEDV` is broken down by `ZN` (treated here as a categorical variable). We can observe that the distribution of `CAT.MEDV` is identical for `ZN` = 17.5, 90, 95, and 100 (where all neighborhoods have `CAT.MEDV = 1`, that is `above30k = Yes`). These four categories can then be combined into a single category. Similarly, categories `ZN` = 12.5, 25, 28, 30, and 70 can be combined.


### Converting a Categorical Variable to a Numerical Variable

Sometimes the categories in a categorical variable represent intervals. Common examples are age group or income bracket. If the interval values are known (e.g., category 2 is the age interval 20-30), we can replace the categorical value ("2" in the example) with the mid-interval value (here "25"). The result will be a numerical variable which no longer requires multiple dummy variables.
